Standart Notebook von unsloth ai verwendet, mit folgenden Änderungen:

// dataset laden + mappen

from datasets import load_dataset
from unsloth.chat_templates import standardize_sharegpt

# 1. Ganze Datei laden
dataset = load_dataset("json", data_files="trainingsdaten3.json")

# 2. In train/test aufteilen (z.B. 90% train, 10% eval)
dataset = dataset["train"].train_test_split(test_size=0.1, seed=42)

# 3. Standardisieren für Unsloth (immer auf den einzelnen Split!)
dataset["train"] = standardize_sharegpt(dataset["train"])
dataset["test"]  = standardize_sharegpt(dataset["test"])

# 4. Chat-Template anwenden
dataset["train"] = dataset["train"].map(formatting_prompts_func, batched=True)
dataset["test"]  = dataset["test"].map(formatting_prompts_func, batched=True)


// Trainingssettings

from trl import SFTConfig, SFTTrainer 
from transformers import DataCollatorForSeq2Seq

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset["train"],   # <-- nur train Split
    eval_dataset = dataset["test"],     # <-- eval Split dazu
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),
    packing = False,
    args = SFTConfig(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        learning_rate = 2e-4,
        logging_steps = 1,
        eval_strategy = "steps",  # <-- eval während Training
        eval_steps = 10,                 # alle 10 Schritte eval
        save_strategy = "steps",         # speichern während Training
        save_steps = 10,
        save_total_limit = 2,            # nur die letzten 2 Saves behalten
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none",
    ),
)
