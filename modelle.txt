V1 = LlaMA 3.2 1B Instruct | 20 Trainingsdaten | JSON Format | 1.json
V2 = LlaMA 3.2 1B Instruct | 20 Trainingsdaten | eigenes Transkript Format | 2.json
V3 = LlaMA 3.2 1B Instruct | 20 Trainingsdaten | eigenes Format | Schnitt Array angepasst statt [[2.34, 3.56], []] zu [(2.34, 3.56), ()] | 3.json
V3.2 = V3 bloß als Q8 quantifiziert
V4 = LlaMA 3.2 3B | 20 Trainingsdaten | alle neuen Formate | 3.json
V5 = V4, bloß im Transkript das Schnitte Format noch erklärt | 3.json
V6 = V4, bloß mit Train/eval wobei 10% für eval sind | 3.json | Train/eval für Training geused s. txt datei 
V7 = V6 | Versuch Overfitting zu vermeiden s. V7_settings.txt
V8 = V6, bloß 50 Daten (30KI/20 Mensch)
V9 = V6 | trainingsdaten3_100_v1 (80KI|20Mensch)
V10 = V6 | trainingsdaten3_100_v2 (100KI|0Mensch)
V11 = Qwen 2.5 7B | trainingsdaten3_100_v1_alpaca | train/eval
V12 = Llama 3.1 8B als Base sonst wie v11
V13 = Misteral_Nemo Base 2407 12B wie v11 sonst
V14 = Phi 4 14B sonst wie v9
V15 = Qwen 3 14B sonst wie v9 (hat reasoning, hab das nicht trainiert) (15_1 ohne reasoning, 15_2 mit reasoning)
V16 = Mistral small Instruct 2409 22B sonst wie v11
V17 = Misteral Nemo Base 2407 12B | 426 Daten (100 KI, 20 selbst, 306 von Max) | train/eval | trainingsdaten_end_alpaca.json & testdaten3_v3_alpaca.json